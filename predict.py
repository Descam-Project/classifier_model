# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G0k3yQbAxuRvmv_PLAHSzVjzXz7M8JRD
"""

!pip install emoji
import pandas as pd
import numpy as np
import io
import re
import nltk
import json
nltk.download('punkt')
from nltk.tokenize import word_tokenize
import emoji
!pip install sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import tokenizer_from_json
import pickle
from flask import Flask, jsonify, request

app = Flask(__name__)

@app.route("/descam/predict", methods=["POST"])

def run_app():
  data = {"success": False, "output": []}
  try:
    params = request.get_json()
    if params is None:
      return jsonify(data)
    if(params != None):
      output = run_prediction(params['input'])

      data["success"] = True
      data["output"] = output
  except:
      print("Get exception")
  return jsonify(data)

def preprocessing(data, tokenizer):
  vocab_size = 50000
  max_length = 500
  trunc_type='post'
  padding_type='post'
  oov_tok = "<OOV>"
  input = data

  input = input.lower() #lowercase
# ilangin tab
  temp = input
  temp = re.sub(r'\n', ' ', temp ) 
#Ilangin angka and simbol
  temp = re.sub('[^a-zA-Z,.?!]+',' ',temp)
#Ilangin Link
  temp = re.sub(r"(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9]+\.[^\s]{2,}|www\.[a-zA-Z0-9]+\.[^\s]{2,})", "", temp)
#Hapus Emoji dsb
  temp = emoji.demojize(temp)
  temp= re.sub(':[A-Za-z_-]+:', ' ', temp) 
# hapus hashtag
  temp= re.sub(r'#(\S+)', r'\1', temp)
# rapihin spasi
  temp = re.sub('[ ]+',' ',temp)
  temp = ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",temp).split())
  input = temp
  
  tokenizer = tokenizer
  input = [input]
  input_sequence = np.array(tokenizer.texts_to_sequences(input))
  input_sequence = pad_sequences(input_sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)
  return input_sequence

def predict(input, model):
  output = ""
  model = model
  pred =  model.predict(input)
  
  if(pred > 0.4):
    output = "Legal"
  else:
    output = "Ilegal"

  return output

def run_prediction(input):
    model = tf.keras.models.load_model("/content/model1.h5")
    with open('/content/tokenizer.json') as f:
      data = json.load(f)
      tokenizer = tokenizer_from_json(data)

    pre = preprocessing(input, tokenizer)
    prediction = predict(pre, model)
    

    return prediction

if __name__ == "__main__":
  app.run(host='0.0.0.0', port=int(os.environ.get("PORT", 8080)))